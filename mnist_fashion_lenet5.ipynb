{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andykhv/notebooks/blob/main/mnist_fashion_lenet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBsfxUJNtF7X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2cIIJjMufkG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIyJItZutV4k",
        "outputId": "e38897c1-d33f-42cd-8cd6-a515c46f2552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 3])\n",
            "tensor([[[1, 1, 1],\n",
            "         [1, 1, 1]],\n",
            "\n",
            "        [[1, 1, 1],\n",
            "         [1, 1, 1]]])\n",
            "tensor([[[0.4027, 0.5700, 0.3110],\n",
            "         [0.4885, 0.4676, 0.1336]],\n",
            "\n",
            "        [[0.6986, 0.9239, 0.7761],\n",
            "         [0.5760, 0.2397, 0.6287]]])\n",
            "tensor([[[0, 0, 0],\n",
            "         [1, 1, 1]],\n",
            "\n",
            "        [[2, 2, 2],\n",
            "         [3, 3, 3]]])\n"
          ]
        }
      ],
      "source": [
        "test = [ [ [0,0,0], [1,1,1] ], [ [2,2,2], [3,3,3] ] ]\n",
        "test = torch.tensor(test)\n",
        "ones = torch.ones_like(test)\n",
        "rands = torch.rand_like(test, dtype=torch.float)\n",
        "\n",
        "print(test.shape)\n",
        "print(ones)\n",
        "print(rands)\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9kOIYPrt5CM",
        "outputId": "5eb121eb-da40-4c25-e772-6c33d38d9413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "cpu\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "print(ones.device)\n",
        "print(rands.device)\n",
        "print(test.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZCTDssMu9KJ",
        "outputId": "09044b39-5cca-47ca-ad16-c812fe6e9bd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "ones.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn47r_C6usDb",
        "outputId": "14800743-b459-4463-e258-4c89a81d39ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1.0000, 1.0000, 1.0000, 0.4027, 0.5700, 0.3110, 0.0000, 0.0000,\n",
            "          0.0000],\n",
            "         [1.0000, 1.0000, 1.0000, 0.4885, 0.4676, 0.1336, 1.0000, 1.0000,\n",
            "          1.0000]],\n",
            "\n",
            "        [[1.0000, 1.0000, 1.0000, 0.6986, 0.9239, 0.7761, 2.0000, 2.0000,\n",
            "          2.0000],\n",
            "         [1.0000, 1.0000, 1.0000, 0.5760, 0.2397, 0.6287, 3.0000, 3.0000,\n",
            "          3.0000]]])\n",
            "torch.Size([2, 2, 9])\n"
          ]
        }
      ],
      "source": [
        "cat = torch.cat([ones, rands, test], dim=2)\n",
        "print(cat)\n",
        "print(cat.shape)\n",
        "\n",
        "# [6,2,3]\n",
        "# [ [[2, 2, 3], [1,1,1]], [[], []], [[], []], [[], []], [[], []], [[], []]]\n",
        "\n",
        "# [2,6,3]\n",
        "# [[[], [], [], [], [], []], [[], [], [], [], [], []]]\n",
        "\n",
        "# [2,2,9]\n",
        "# [ [ 1, 1, 1, ], [  ] ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NcsEWHxMqgZ",
        "outputId": "3a9e1ae4-df0b-4652-f46f-ea381aa9e1e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 1.0000, 1.0000, 0.6986, 0.9239, 0.7761, 2.0000, 2.0000,\n",
              "          2.0000],\n",
              "         [1.0000, 1.0000, 1.0000, 0.5760, 0.2397, 0.6287, 3.0000, 3.0000,\n",
              "          3.0000]],\n",
              "\n",
              "        [[1.0000, 1.0000, 1.0000, 0.6986, 0.9239, 0.7761, 2.0000, 2.0000,\n",
              "          2.0000],\n",
              "         [1.0000, 1.0000, 1.0000, 0.5760, 0.2397, 0.6287, 3.0000, 3.0000,\n",
              "          3.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "input = [1,1]\n",
        "cat[input]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4bXRPJuM717",
        "outputId": "66c3f4a2-24b5-4cd8-91e4-faba7900e75d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1., -1.,  1.,  1.], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
        "out = x.sum()\n",
        "out.backward()\n",
        "x.grad\n",
        "\n",
        "x.view(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o7cjwBoNhSW",
        "outputId": "d80ab890-98d0-4c2f-c275-90c13df7462a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 12404594.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 209622.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3930863.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 19593717.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5),(0.5))])\n",
        "\n",
        "#create datasets\n",
        "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
        "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyohXIDLdTJN",
        "outputId": "2a2f9460-ed39-4229-bd67-b4bbba930767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=0.5, std=0.5)\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dObm3IogUK9U",
        "outputId": "eda670cc-7595-4119-d88d-ee15389a0ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set has 60000 instances\n",
            "Validation set has 10000 instances\n"
          ]
        }
      ],
      "source": [
        "#create loaders\n",
        "training_loader = torch.utils.data.DataLoader(training_set, shuffle=True, batch_size=4)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, shuffle=False, batch_size=4)\n",
        "\n",
        "#class labels\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "#Report split sizes\n",
        "print('Training set has {} instances'.format(len(training_set)))\n",
        "print('Validation set has {} instances'.format(len(validation_set)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmqMiSzkVvp8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Helper function for inline image display\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apjgiy9gWdd2"
      },
      "outputs": [],
      "source": [
        "#training_iter = iter(training_loader)\n",
        "#images, labels = next(training_iter)\n",
        "\n",
        "#create grid from the images and show them\n",
        "#img_grid = torchvision.utils.make_grid(images)\n",
        "#matplotlib_imshow(img_grid, one_channel=True)\n",
        "#print(' '.join(classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMkr2N63Rhry"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GarmentClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GarmentClassifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "    self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "    self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 16 * 4 * 4)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "model = GarmentClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRGbvU1qTPdC",
        "outputId": "1046311b-7192-47df-eaf3-74d615157fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3768, 0.9489, 0.6123, 0.2639, 0.4472, 0.4284, 0.8145, 0.7718, 0.7461,\n",
            "         0.0123],\n",
            "        [0.2940, 0.1155, 0.7632, 0.6275, 0.0470, 0.9756, 0.3935, 0.4651, 0.5228,\n",
            "         0.9260],\n",
            "        [0.5804, 0.5233, 0.3657, 0.8335, 0.0576, 0.3730, 0.2103, 0.9246, 0.7134,\n",
            "         0.4346],\n",
            "        [0.4212, 0.1212, 0.9134, 0.9378, 0.5017, 0.1880, 0.1075, 0.5244, 0.4221,\n",
            "         0.0489]])\n",
            "tensor([1, 5, 3, 7])\n"
          ]
        }
      ],
      "source": [
        "#loss functions expect data in batches, we will create batches of 4\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#represents the model's confidence in each of the 10 classes for a given input\n",
        "dummy_outputs = torch.rand(4, 10)\n",
        "#represents the correct class among the 10 being tested\n",
        "dummy_labels = torch.tensor([1,5,3,7])\n",
        "\n",
        "print(dummy_outputs)\n",
        "print(dummy_labels)\n",
        "\n",
        "#loss = loss_fn(dummy_outputs, dummy_labels)\n",
        "#print('Total loss for this batch: {}'.format(loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM4fxGiOU15q"
      },
      "outputs": [],
      "source": [
        "#Optimizers specified in the torch.optim package\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6_VH8cPpZgw"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "  running_loss = 0.\n",
        "  last_loss = 0.\n",
        "\n",
        "  for count, (inputs, labels) in enumerate(training_loader):\n",
        "    optimizer.zero_grad() #zero gradients every batch\n",
        "    outputs = model(inputs) #forward pass\n",
        "\n",
        "    #compute loss and its gradients\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    #adjust learning weights\n",
        "    optimizer.step()\n",
        "\n",
        "    #gather data and report\n",
        "    running_loss += loss.item()\n",
        "    if count % 1000 == 999:\n",
        "      last_loss = running_loss / 1000 #loss per batch\n",
        "      print('batch {}, loss: {}'.format(count + 1, last_loss))\n",
        "      tb_x = epoch_index * len(training_loader) + count + 1\n",
        "      tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "      running_loss = 0\n",
        "\n",
        "  return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRn6yy90rOE4",
        "outputId": "dcc4613a-7f5b-471f-c179-eda8aaf59d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "batch 1000, loss: 2.0500668913722038\n",
            "batch 2000, loss: 0.8527674072533846\n",
            "batch 3000, loss: 0.6661956694293767\n",
            "batch 4000, loss: 0.6016246316577308\n",
            "batch 5000, loss: 0.5593955060234294\n",
            "batch 6000, loss: 0.5332814521880355\n",
            "batch 7000, loss: 0.5079441717648878\n",
            "batch 8000, loss: 0.507990576643264\n",
            "batch 9000, loss: 0.474091516426357\n",
            "batch 10000, loss: 0.43986326645582446\n",
            "batch 11000, loss: 0.45774787388468396\n",
            "batch 12000, loss: 0.4233336500849109\n",
            "batch 13000, loss: 0.4066347028008895\n",
            "batch 14000, loss: 0.42871725975826847\n",
            "batch 15000, loss: 0.4005424371238259\n",
            "LOSS train 0.4005424371238259 valid 0.42974355816841125\n",
            "EPOCH 2:\n",
            "batch 1000, loss: 0.3858801536022802\n",
            "batch 2000, loss: 0.3610106304203946\n",
            "batch 3000, loss: 0.37806677814817524\n",
            "batch 4000, loss: 0.36983968287854807\n",
            "batch 5000, loss: 0.3624696763869724\n",
            "batch 6000, loss: 0.3615973337913747\n",
            "batch 7000, loss: 0.36974083856516515\n",
            "batch 8000, loss: 0.3673414829936519\n",
            "batch 9000, loss: 0.36293719217198667\n",
            "batch 10000, loss: 0.3727502234059502\n",
            "batch 11000, loss: 0.3554063735244781\n",
            "batch 12000, loss: 0.35338571395998586\n",
            "batch 13000, loss: 0.34404387592282726\n",
            "batch 14000, loss: 0.3541534711603308\n",
            "batch 15000, loss: 0.3518503449663549\n",
            "LOSS train 0.3518503449663549 valid 0.3604460060596466\n",
            "EPOCH 3:\n",
            "batch 1000, loss: 0.3356829811821808\n",
            "batch 2000, loss: 0.30761505722285076\n",
            "batch 3000, loss: 0.3527585404236452\n",
            "batch 4000, loss: 0.331733719525022\n",
            "batch 5000, loss: 0.3235016632308398\n",
            "batch 6000, loss: 0.3258321655227137\n",
            "batch 7000, loss: 0.32501239336702564\n",
            "batch 8000, loss: 0.2962059482554614\n",
            "batch 9000, loss: 0.33145360466106105\n",
            "batch 10000, loss: 0.32428187561470984\n",
            "batch 11000, loss: 0.31704578590148597\n",
            "batch 12000, loss: 0.2946739743402277\n",
            "batch 13000, loss: 0.3117428073089759\n",
            "batch 14000, loss: 0.3115324583209076\n",
            "batch 15000, loss: 0.31142742356946107\n",
            "LOSS train 0.31142742356946107 valid 0.3441337049007416\n",
            "EPOCH 4:\n",
            "batch 1000, loss: 0.279650096458794\n",
            "batch 2000, loss: 0.2975610666997491\n",
            "batch 3000, loss: 0.2822205988376024\n",
            "batch 4000, loss: 0.2990531960692679\n",
            "batch 5000, loss: 0.2957510293344667\n",
            "batch 6000, loss: 0.30241083427185367\n",
            "batch 7000, loss: 0.30967758671406775\n",
            "batch 8000, loss: 0.2865275486935279\n",
            "batch 9000, loss: 0.31093525807098193\n",
            "batch 10000, loss: 0.2918675608582489\n",
            "batch 11000, loss: 0.315983315923193\n",
            "batch 12000, loss: 0.2858283314065193\n",
            "batch 13000, loss: 0.28693267934570393\n",
            "batch 14000, loss: 0.3046802762936786\n",
            "batch 15000, loss: 0.27493795607759514\n",
            "LOSS train 0.27493795607759514 valid 0.3369758427143097\n",
            "EPOCH 5:\n",
            "batch 1000, loss: 0.2765945276746279\n",
            "batch 2000, loss: 0.252476597289904\n",
            "batch 3000, loss: 0.2684856752890046\n",
            "batch 4000, loss: 0.27067613047742817\n",
            "batch 5000, loss: 0.2666815358554195\n",
            "batch 6000, loss: 0.28564055501645635\n",
            "batch 7000, loss: 0.2797304217051278\n",
            "batch 8000, loss: 0.26795710016828705\n",
            "batch 9000, loss: 0.27946583895638083\n",
            "batch 10000, loss: 0.2577760647300511\n",
            "batch 11000, loss: 0.25129748257772005\n",
            "batch 12000, loss: 0.28349132370267033\n",
            "batch 13000, loss: 0.2729357897931859\n",
            "batch 14000, loss: 0.29317749362836915\n",
            "batch 15000, loss: 0.2850201472872277\n",
            "LOSS train 0.2850201472872277 valid 0.3548906147480011\n"
          ]
        }
      ],
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/lenet5_maxpool_relu')\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "  #gradient tracking on, do a pass over data\n",
        "  model.train(True)\n",
        "  avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "  #set model to evaluation mode, disabling dropout and using population\n",
        "  #statistics for batch normalization\n",
        "  model.eval()\n",
        "\n",
        "  #disable gradient computation and reduce memory consumption\n",
        "  running_vloss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for i, vdata in enumerate(validation_loader):\n",
        "      vinputs, vlabels = vdata\n",
        "      voutputs = model(vinputs)\n",
        "      vloss = loss_fn(voutputs, vlabels)\n",
        "      running_vloss += vloss\n",
        "\n",
        "  avg_vloss = running_vloss / (i + 1)\n",
        "  print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "  #log running loss averaged per batch for both training and validation\n",
        "  writer.add_scalars('Training vs. Validation Loss',\n",
        "                     { 'Training': avg_loss, 'Validation': avg_vloss},\n",
        "                     epoch_number + 1)\n",
        "  writer.flush()\n",
        "\n",
        "  #track best performance, and save model's state\n",
        "  if avg_vloss < best_vloss:\n",
        "    best_vloss = avg_vloss\n",
        "    model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "  epoch_number += 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1MWCyjAQ73R2hD1-1Owx-gVTKa51mAUcQ",
      "authorship_tag": "ABX9TyM4mW/2f/v9SM2XtZEG5xe8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}